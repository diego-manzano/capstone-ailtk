{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Monitor and improve Virtual Agent performance through user satisfaction ratings and feedback\n",
    "##### (GenAI Life Cycle Phase 7: Monitoring and Improvement self-practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case Scenario**\n",
    ">\n",
    "> CoffeePro’s virtual coffee concierge is now live and assisting users with personalized coffee recommendations. However, launching the virtual agent is just the beginning—ensuring its continuous improvement based on real user interactions is crucial. As users engage with the AI, their feedback, ratings, and behavioral data provide valuable insights into the assistant’s effectiveness, accuracy, and user satisfaction.\n",
    "> \n",
    "> As the AI developer, your task is to implement a monitoring system that collects and analyzes feedback, allowing you to refine the virtual agent’s performance over time. This includes tracking response accuracy, identifying patterns in user satisfaction, and leveraging Retrieval-Augmented Generation (RAG) enhancements to improve recommendations. Additionally, the system should detect recurring issues, such as misinterpretations, biases, or inadequate responses, and provide mechanisms for updating the model accordingly.\n",
    "> \n",
    "> Your Tasks:\n",
    ">\n",
    "> (a) Analyze feedback data \n",
    "Develop a structured approach to evaluate feedback trends, detect areas for improvement, and update the knowledge base accordingly.\n",
    "> \n",
    "> (b) Refine model performance\n",
    "Utilize insights from user ratings and comments to enhance response accuracy, personalize recommendations, and optimize the virtual agent’s conversational experience.\n",
    ">\n",
    "> By the end of this activity, you will have gained practical experience in monitoring AI performance, analyzing user feedback, and implementing continuous improvements to ensure CoffeePro’s virtual agent remains effective, reliable, and user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Analyze feedback data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Access Visual Studio Code and the Jupyter Notebook prepared for this Practice Learning Activity <a href=\"../learning-files/ailtk-running-code-pla7.ipynb\" target=\"_blank\">(Click here to open Workbook: Practice Learning Activity 7 in Visual Studio Code)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Provided to you is a spreadsheet `exported_data.xlsx` file containing exported Virtual Agent feedback records from a MySQL database. Run the code cell below to load the file into a pandas dataframe for our further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>feedback_type</th>\n",
       "      <th>additional_feedback</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Provide an example recipe for a tall black coffee</td>\n",
       "      <td>&lt;p&gt;There's no recipe for a tall black coffee b...</td>\n",
       "      <td>thumbs-up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-28 14:24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>hello</td>\n",
       "      <td>&lt;p&gt;Hello there! How can I help you today?&lt;/p&gt;\\n</td>\n",
       "      <td>thumbs-down</td>\n",
       "      <td>test entry</td>\n",
       "      <td>2025-01-28 14:26:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>How can I get a refund for my coffee order?</td>\n",
       "      <td>&lt;p&gt;Sorry, I’m not equipped to provide refund d...</td>\n",
       "      <td>thumbs-down</td>\n",
       "      <td>Needs better customer service integration</td>\n",
       "      <td>2025-01-28 14:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Can I return a defective coffee maker?</td>\n",
       "      <td>&lt;p&gt;I'm sorry, but I don’t have information on ...</td>\n",
       "      <td>thumbs-down</td>\n",
       "      <td>Frustrating experience</td>\n",
       "      <td>2025-01-28 14:25:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>How do I track my order?</td>\n",
       "      <td>&lt;p&gt;Apologies, but I can’t access order trackin...</td>\n",
       "      <td>thumbs-down</td>\n",
       "      <td>Would be helpful if it could track orders</td>\n",
       "      <td>2025-01-28 14:30:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt  \\\n",
       "0   1  Provide an example recipe for a tall black coffee   \n",
       "1   2                                              hello   \n",
       "2   5        How can I get a refund for my coffee order?   \n",
       "3   6             Can I return a defective coffee maker?   \n",
       "4   7                           How do I track my order?   \n",
       "\n",
       "                                            response feedback_type  \\\n",
       "0  <p>There's no recipe for a tall black coffee b...     thumbs-up   \n",
       "1    <p>Hello there! How can I help you today?</p>\\n   thumbs-down   \n",
       "2  <p>Sorry, I’m not equipped to provide refund d...   thumbs-down   \n",
       "3  <p>I'm sorry, but I don’t have information on ...   thumbs-down   \n",
       "4  <p>Apologies, but I can’t access order trackin...   thumbs-down   \n",
       "\n",
       "                         additional_feedback          created_at  \n",
       "0                                        NaN 2025-01-28 14:24:06  \n",
       "1                                 test entry 2025-01-28 14:26:38  \n",
       "2  Needs better customer service integration 2025-01-28 14:20:00  \n",
       "3                     Frustrating experience 2025-01-28 14:25:45  \n",
       "4  Would be helpful if it could track orders 2025-01-28 14:30:10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# LOAD EXCEL INTO DATAFRAME ----\n",
    "excel_path = \"../exported_data.xlsx\"\n",
    "df = pd.read_excel(excel_path)  # Read Excel into a DataFrame\n",
    "\n",
    "# PRINT THE FIRST FEW ROWS ----\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the head of the database we can see that it contains six columns: `id`, `prompt`, `response`, `feedback_type`, `additional_feedback`, and `created_at`. Three of the columns (`prompt`, `response`, and `additional_feedback`) are longer text entries. We can use wordclouds as tools for these types of data.\n",
    "- Word clouds offer a quick visual overview of frequently used terms in LLP prompts, responses, and feedback, highlighting key themes and areas of focus. This is a useful tool for identifying dominant topics, recurring issues, or sentiment trends.  While helpful for summarizing large volumes of text, word clouds are more often that not used in conjunction with more detailed analysis to retain context and avoid misinterpretations.\n",
    "- One thing to note is that `additional_feedback` is only collected for `thumbs-down` entries. Typically, feedback systems are designed to prioritize negative feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. First, we make a wordcloud for the `prompt` column by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      6\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m wordcloud_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m, colormap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate(prompt_text)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud_prompt, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ---- WORD CLOUD: PROMPTS ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "prompt_text = \" \".join(df[\"prompt\"].dropna().astype(str))\n",
    "wordcloud_prompt = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"viridis\").generate(prompt_text)\n",
    "plt.imshow(wordcloud_prompt, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Prompts\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Second, we make a wordcloud for the `response` column by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- WORD CLOUD: RESPONSES ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "response_text = \" \".join(df[\"response\"].dropna().astype(str))\n",
    "wordcloud_response = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"magma\").generate(response_text)\n",
    "plt.imshow(wordcloud_response, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Responses\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our primary concern when checking this column is whether or not the Virtual Agent has been safe and user-friendly. We can check this using Detoxify, similarly to what we previously did in Competency 5 (Evaluate models on use cases and for safety). Run the code cell below to compute for the toxicity scores of each entry and generate a heat map. Note that this may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from detoxify import Detoxify\n",
    "\n",
    "# Initialize Detoxify model\n",
    "detoxify_model = Detoxify('original')\n",
    "\n",
    "# Parameters\n",
    "toxicity_threshold = 0.5  # Threshold for flagging toxicity\n",
    "toxicity_scores_list = []  # List to store toxicity scores\n",
    "\n",
    "# Assuming df is your DataFrame, iterate over each row in the 'response' column\n",
    "for i, response in enumerate(df['response']):\n",
    "    # Evaluate the response for toxicity using Detoxify\n",
    "    toxicity_scores = detoxify_model.predict(response)\n",
    "    \n",
    "    # Ensure scores are converted to standard Python float\n",
    "    toxicity_scores = {key: float(value) for key, value in toxicity_scores.items()}\n",
    "    print(f\"Toxicity Scores for response {i}: {toxicity_scores}\")\n",
    "    \n",
    "    # Store toxicity scores for visualization\n",
    "    toxicity_scores_list.append(toxicity_scores)\n",
    "    \n",
    "    # Flagging responses with high toxicity or other unsafe attributes\n",
    "    if any(score > toxicity_threshold for score in toxicity_scores.values()):\n",
    "        print(f\"Warning: Potentially unsafe content detected in response {i}.\")\n",
    "        print(f\"Details: {toxicity_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the toxicity scores by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of toxicity scores to a DataFrame\n",
    "toxicity_df = pd.DataFrame(toxicity_scores_list)\n",
    "\n",
    "# Set up the heatmap plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    toxicity_df, \n",
    "    annot=True,  \n",
    "    cmap= sns.color_palette(\"coolwarm\", as_cmap=True),\n",
    "    vmin=0,  # Minimum value\n",
    "    vmax=1,  # Maximum value\n",
    "    cbar=True)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Toxicity Scores Heatmap')\n",
    "plt.xlabel('Toxicity Categories')\n",
    "plt.ylabel('Responses')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the Toxicity Score Heatmap we can see that none of the Virtual Agent's responses are problematic from a safety perspective. Since we are using a pre-trained model (Google Gemini), this result is expected, as such implementations undergo rigorous safety evaluations to mitigate the risk of generating toxic or harmful content.  However, it's still crucial to monitor and evaluate the model's performance in our specific use case to ensure continued safety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The next column of intreset is the `feedback_type` distribution. From the head of the dataframe we were able to see that the entries consisted of either `thumbs-up` or `thumbs-down`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- BAR GRAPH: 'thumbs-up' vs 'thumbs-down' ----\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x=\"feedback_type\", palette={\"thumbs-up\": \"green\", \"thumbs-down\": \"red\"})\n",
    "plt.title(\"Feedback Distribution\")\n",
    "plt.xlabel(\"Feedback Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Majority of the entries are positive (thumbs-up). Regardless, we should look into the negative (thumbs-down) to find any possible issues. We do so by looking further into the next column: `additional_feedback`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The next column of interest is the `additional_feedback`. We can give ourselves an idea of its contents by generating another word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- WORD CLOUD: ADDITIONAL FEEDBACK ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Drop NaN entries\n",
    "feedback_text = \" \".join(df[\"additional_feedback\"].dropna().astype(str))\n",
    "\n",
    "wordcloud_feedback = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"plasma\").generate(feedback_text)\n",
    "plt.imshow(wordcloud_feedback, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Additional Feedback\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see some recurring words that could be of interest, signaling possible gaps and improvements to be made. Given this, we can use further methods to further understand the data present here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We start our further analysis of the `additional_feedback` by preprocessing its entries. Run the code below to use the nltk library and preprocess the column's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and non-alphanumeric characters\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to each feedback entry\n",
    "df_cleaned = df['additional_feedback'].dropna().apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we apply n-gram analysis to identify common word pairs (bigrams) in the preprocessed feedback data. Run the following code to extract and display the most frequent bigrams. N-gram analysis is a natural language processing technique that examines contiguous sequences of n words in a text. For example, bigrams (n=2) look at word pairs, while trigrams (n=3) analyze sequences of three words. This approach helps identify common phrases, patterns, and recurring themes in textual data. In our case, n-grams can highlight frequently mentioned concerns, praise, or issues, providing valuable insights into customer sentiment and recurring topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a bigram model (you can change ngram_range for different n-grams)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "X = vectorizer.fit_transform(df_cleaned)\n",
    "\n",
    "# Get the most frequent n-grams\n",
    "ngram_freq = X.toarray().sum(axis=0)\n",
    "ngram_terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with n-grams and their frequencies\n",
    "ngram_df = pd.DataFrame(list(zip(ngram_terms, ngram_freq)), columns=[\"Bigram\", \"Frequency\"])\n",
    "ngram_df = ngram_df.sort_values(by=\"Frequency\", ascending=False)\n",
    "\n",
    "# Display the top 10 most frequent n-grams\n",
    "print(ngram_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here, we can see some bigrams of concern ('customer service' and 'need better'). Run the code cell below to view the entries containing those bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bigrams to search for\n",
    "bigrams_to_check = ['customer service', 'needs better']\n",
    "\n",
    "# Function to check if any bigram is in a text, and ensure text is a string\n",
    "def contains_bigram(text, bigrams):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        return any(bigram in text for bigram in bigrams)\n",
    "    return False  # Return False if it's not a string\n",
    "\n",
    "# Apply the check directly to the 'additional_feedback' column, ensuring no NaN values\n",
    "filtered_df = df[df['additional_feedback'].notna() & df['additional_feedback'].apply(lambda x: contains_bigram(x, bigrams_to_check))]\n",
    "\n",
    "# Display the filtered entries\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can see the specific feedback entries related to the bigrams of concern, allowing us to directly address these as potential points for improving the Virtual Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Refine model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From analyzing the feedback data, what are gaps in the Virtual Agent that are affecting customer experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c5e2eaa4d947e8b331ce85a0e745da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Type your answer here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6373eac82f164f54a391b9f2b802bd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Answer:', layout=Layout(height='100px', width='500px'), placeholder='Type your…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f984a109094451ba0542e5b9917e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4356d4987c74816b551dfb40baf50aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input text box\n",
    "input_box1 = widgets.Textarea(\n",
    "    placeholder='Type your answer here...',\n",
    "    description='Answer:',\n",
    "    layout=widgets.Layout(width='500px', height='100px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button1 = widgets.Button(\n",
    "    description=\"Submit\",\n",
    "    button_style='primary'  # Optional: styling\n",
    ")\n",
    "\n",
    "# Create output widget\n",
    "output1 = widgets.Output()\n",
    "\n",
    "# Define the button click event\n",
    "def on_submit_click1(b):\n",
    "    # Clear previous output\n",
    "    output1.clear_output()\n",
    "    with output1:\n",
    "        print(\"\"\"\n",
    "    Analysis of user feedback reveals a significant gap between the \n",
    "    intended function of the virtual agent and how users are actually \n",
    "    interacting with it.  While designed to serve as a virtual coffee \n",
    "    concierge, guiding users through orders and product information, \n",
    "    the feedback clearly indicates that users are primarily looking to \n",
    "    the agent for customer service support, particularly regarding \n",
    "    issues like refunds, lost packages, and general assistance.  \n",
    "              \n",
    "    This mismatch highlights a possible need to expand the agent's \n",
    "    capabilities beyond its current scope.\n",
    "    \"\"\")\n",
    "\n",
    "# Set the button's on-click function\n",
    "submit_button1.on_click(on_submit_click1)\n",
    "\n",
    "# Display the widgets\n",
    "display(input_box1, submit_button1, output1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a new problem has arisen—users seeking customer service—and the GenAI Lifecycle returns to its first phase: Problem Definition. Development is rarely linear, requiring iterative cycles of stakeholder realignment and revisiting each phase to improve the Virtual Agent over several iterations. Equipped with the competencies from this learning toolkit, you will be able to apply the knowledge and skills that you learned to build Virtual Agents that can remain relevant to evolving customer and business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You’ve now completed the final Practice Learning Activity of this self-learning toolkit, applying the essential skills needed to monitor and improve your virtual agent through user feedback. By implementing structured evaluation systems, analyzing satisfaction ratings, and refining model performance, you’ve gained hands-on experience in ensuring AI systems remain accurate, ethical, and user-friendly over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Back to Learning Instructions 7](../learning-instructions-7.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
