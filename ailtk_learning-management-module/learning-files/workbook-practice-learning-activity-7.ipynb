{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook\n",
    "# (7) Practice Learning Activity: Monitor and improve Virtual Agent performance through user satisfaction ratings and feedback\n",
    "##### (GenAI Life Cycle Phase 7: Monitoring and Improvement self-practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Provided to you is a spreadsheet `exported_data.xlsx` file containing exported Virtual Agent feedback records from a MySQL database. Run the code cell below to load the file into a pandas dataframe for our further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# LOAD EXCEL INTO DATAFRAME ----\n",
    "excel_path = \"/home/ailtk-learner/Documents/GitHub/capstone-ailtk/ailtk_learning-management-module/learning-files/exported_data.xlsx\"\n",
    "df = pd.read_excel(excel_path)  # Read Excel into a DataFrame\n",
    "\n",
    "# PRINT THE FIRST FEW ROWS ----\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. First, we make a wordcloud for the `prompt` column by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ---- WORD CLOUD: PROMPTS ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "prompt_text = \" \".join(df[\"prompt\"].dropna().astype(str))\n",
    "wordcloud_prompt = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"viridis\").generate(prompt_text)\n",
    "plt.imshow(wordcloud_prompt, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Prompts\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- WORD CLOUD: PROMPTS ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "prompt_text = \" \".join(df[\"prompt\"].dropna().astype(str))\n",
    "wordcloud_prompt = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"viridis\").generate(prompt_text)\n",
    "plt.imshow(wordcloud_prompt, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Prompts\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- WORD CLOUD: RESPONSES ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "response_text = \" \".join(df[\"response\"].dropna().astype(str))\n",
    "wordcloud_response = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"magma\").generate(response_text)\n",
    "plt.imshow(wordcloud_response, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Responses\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our primary concern when checking this column is whether or not the Virtual Agent has been safe and user-friendly. We can check this using Detoxify, similarly to what we previously did in Competency 5 (Evaluate models on use cases and for safety). Run the code cell below to compute for the toxicity scores of each entry and generate a heat map. Note that this may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from detoxify import Detoxify\n",
    "\n",
    "# Initialize Detoxify model\n",
    "detoxify_model = Detoxify('original')\n",
    "\n",
    "# Parameters\n",
    "toxicity_threshold = 0.5  # Threshold for flagging toxicity\n",
    "toxicity_scores_list = []  # List to store toxicity scores\n",
    "\n",
    "# Assuming df is your DataFrame, iterate over each row in the 'response' column\n",
    "for i, response in enumerate(df['response']):\n",
    "    # Evaluate the response for toxicity using Detoxify\n",
    "    toxicity_scores = detoxify_model.predict(response)\n",
    "    \n",
    "    # Ensure scores are converted to standard Python float\n",
    "    toxicity_scores = {key: float(value) for key, value in toxicity_scores.items()}\n",
    "    print(f\"Toxicity Scores for response {i}: {toxicity_scores}\")\n",
    "    \n",
    "    # Store toxicity scores for visualization\n",
    "    toxicity_scores_list.append(toxicity_scores)\n",
    "    \n",
    "    # Flagging responses with high toxicity or other unsafe attributes\n",
    "    if any(score > toxicity_threshold for score in toxicity_scores.values()):\n",
    "        print(f\"Warning: Potentially unsafe content detected in response {i}.\")\n",
    "        print(f\"Details: {toxicity_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the toxicity scores by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of toxicity scores to a DataFrame\n",
    "toxicity_df = pd.DataFrame(toxicity_scores_list)\n",
    "\n",
    "# Set up the heatmap plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    toxicity_df, \n",
    "    annot=True,  \n",
    "    cmap= sns.color_palette(\"coolwarm\", as_cmap=True),\n",
    "    vmin=0,  # Minimum value\n",
    "    vmax=1,  # Maximum value\n",
    "    cbar=True)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Toxicity Scores Heatmap')\n",
    "plt.xlabel('Toxicity Categories')\n",
    "plt.ylabel('Responses')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the Toxicity Score Heatmap we can see that none of the Virtual Agent's responses are problematic from a safety perspective. Since we are using a pre-trained model (Google Gemini), this result is expected, as such implementations undergo rigorous safety evaluations to mitigate the risk of generating toxic or harmful content.  However, it's still crucial to monitor and evaluate the model's performance in our specific use case to ensure continued safety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The next column of intreset is the `feedback_type` distribution. From the head of the dataframe we were able to see that the entries consisted of either `thumbs-up` or `thumbs-down`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- BAR GRAPH: 'thumbs-up' vs 'thumbs-down' ----\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x=\"feedback_type\", palette={\"thumbs-up\": \"green\", \"thumbs-down\": \"red\"})\n",
    "plt.title(\"Feedback Distribution\")\n",
    "plt.xlabel(\"Feedback Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Majority of the entries are positive (thumbs-up). Regardless, we should look into the negative (thumbs-down) to find any possible issues. We do so by looking further into the next column: `additional_feedback`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The next column of interest is the `additional_feedback`. We can give ourselves an idea of its contents by generating another word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- WORD CLOUD: ADDITIONAL FEEDBACK ----\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Drop NaN entries\n",
    "feedback_text = \" \".join(df[\"additional_feedback\"].dropna().astype(str))\n",
    "\n",
    "wordcloud_feedback = WordCloud(width=600, height=400, background_color=\"white\", colormap=\"plasma\").generate(feedback_text)\n",
    "plt.imshow(wordcloud_feedback, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Additional Feedback\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see some recurring words that could be of interest, signaling possible gaps and improvements to be made. Given this, we can use further methods to further understand the data present here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We start our further analysis of the `additional_feedback` by preprocessing its entries. Run the code below to use the nltk library and preprocess the column's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and non-alphanumeric characters\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to each feedback entry\n",
    "df_cleaned = df['additional_feedback'].dropna().apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we apply n-gram analysis to identify common word pairs (bigrams) in the preprocessed feedback data. Run the following code to extract and display the most frequent bigrams. N-gram analysis is a natural language processing technique that examines contiguous sequences of n words in a text. For example, bigrams (n=2) look at word pairs, while trigrams (n=3) analyze sequences of three words. This approach helps identify common phrases, patterns, and recurring themes in textual data. In our case, n-grams can highlight frequently mentioned concerns, praise, or issues, providing valuable insights into customer sentiment and recurring topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a bigram model (you can change ngram_range for different n-grams)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "X = vectorizer.fit_transform(df_cleaned)\n",
    "\n",
    "# Get the most frequent n-grams\n",
    "ngram_freq = X.toarray().sum(axis=0)\n",
    "ngram_terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with n-grams and their frequencies\n",
    "ngram_df = pd.DataFrame(list(zip(ngram_terms, ngram_freq)), columns=[\"Bigram\", \"Frequency\"])\n",
    "ngram_df = ngram_df.sort_values(by=\"Frequency\", ascending=False)\n",
    "\n",
    "# Display the top 10 most frequent n-grams\n",
    "print(ngram_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here, we can see some bigrams of concern ('customer service' and 'need better'). Run the code cell below to view the entries containing those bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bigrams to search for\n",
    "bigrams_to_check = ['customer service', 'needs better']\n",
    "\n",
    "# Function to check if any bigram is in a text, and ensure text is a string\n",
    "def contains_bigram(text, bigrams):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        return any(bigram in text for bigram in bigrams)\n",
    "    return False  # Return False if it's not a string\n",
    "\n",
    "# Apply the check directly to the 'additional_feedback' column, ensuring no NaN values\n",
    "filtered_df = df[df['additional_feedback'].notna() & df['additional_feedback'].apply(lambda x: contains_bigram(x, bigrams_to_check))]\n",
    "\n",
    "# Display the filtered entries\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bigrams to search for\n",
    "bigrams_to_check = ['customer service', 'needs better']\n",
    "\n",
    "# Function to check if any bigram is in a text, and ensure text is a string\n",
    "def contains_bigram(text, bigrams):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        return any(bigram in text for bigram in bigrams)\n",
    "    return False  # Return False if it's not a string\n",
    "\n",
    "# Apply the check directly to the 'additional_feedback' column, ensuring no NaN values\n",
    "filtered_df = df[df['additional_feedback'].notna() & df['additional_feedback'].apply(lambda x: contains_bigram(x, bigrams_to_check))]\n",
    "\n",
    "# Display the filtered entries\n",
    "filtered_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
