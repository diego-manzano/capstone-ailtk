{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook\n",
    "## (2) Practice Learning Activity: Source and investigate usable data sources \n",
    "##### (GenAI Life Cycle Phase 2: Data Understanding self-practice)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. Run the code segment below in a Python code cell to import the necessary Python modules by clicking the button (![image.png](attachment:image.png)) to the left of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlalchemy\n",
    "from sqlalchemy import inspect\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now that the necessary Python modules are uploaded we can now proceed to wokring with the MySQL Database from our scenario. Here are the credentials that you can use to login to the MySQL server:\n",
    "\n",
    "      | **Username:** | `ailtk-learner`|\n",
    "      | **Password:** | `DLSU1234!`    |\n",
    "\n",
    "   - The database for this practice learning activity is `ailtk_db`.\n",
    "\n",
    "   Run the code segment below first establish connection to MySQL server and retrieve the databse's table names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MySQL Database\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://ailtk-learner:DLSU1234!@localhost:3306/ailtk_db')\n",
    "\n",
    "# Inspect the database to get the table names\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Print the table names\n",
    "print(\"Tables in the database:\", table_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "6.  Run the code segment belows in Python code cells to load data and print basic statistics for our Exploratory Data Analysis for each of the tables.\n",
    "    - Inspect the content of the SQL tables by printing the head of the data. This can be done by loading the table into a pandas dataframe and using the built-in `df.head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tablename to select\n",
    "table_name = \"products_beans\"\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tablename \n",
    "table_name = \"roasts\"\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tablename \n",
    "table_name = \"products_beans_reviews\"\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tablename \n",
    "table_name = \"products_beans_origins\"\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tablename \n",
    "table_name = \"roasters\"\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tablename \n",
    "table_name = \"roasters_countries\"\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Next we look at distributions of categorical data. In this example, categorical variables primarily entail columns of the `product_beans` featuring two or more categories of which have no intrinsic ordering. An example of this are the roasters found in the database, featuring several unique emtroes. Run the code cell below to view the total number of roasters and the of which are unique.  \n",
    "\n",
    "    *[Review categorical, ordinal and interval variables here.](https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/)*\n",
    "\n",
    "- We can first look at the number of `product_beans` rows in relationship to the `roasters` table, which we seen is referenced through the \"roaster_id\" column in the former table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'roasters' table\n",
    "query = \"SELECT * FROM roasters\"\n",
    "roasters_df = pd.read_sql(query, engine)\n",
    "\n",
    "# Load the 'roasters_countries' table\n",
    "query = \"SELECT * FROM roasters_countries\"\n",
    "countries_df = pd.read_sql(query, engine)\n",
    "\n",
    "# Show first few rows\n",
    "display(roasters_df.head())\n",
    "display(countries_df.head())\n",
    "\n",
    "# Calculate total number of roasters and unique roasters\n",
    "total_roasters = roasters_df['roaster_id'].count()\n",
    "total_countries = countries_df['country_id'].count()\n",
    "\n",
    "print(\"Total number of emtries im `roasters`:\", total_roasters)\n",
    "print(\"Total number of emtries im `roasters_countries`:\", total_countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. *(cont.)* We can look at the `roasters` table further by grouping them by the dimension of the `roasters_countries`. Observe the distribution of the total number of `roasters` by `roasters_countries` by running the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge roasters with countries to get country names\n",
    "roasters_with_countries = pd.merge(roasters_df, countries_df, on=\"country_id\", how=\"left\")\n",
    "\n",
    "# Calculate number of roasters per country\n",
    "roasters_per_country = roasters_with_countries.groupby(\"roaster_country\")['roaster_id'].count().reset_index()\n",
    "roasters_per_country.columns = ['Country', 'Number of Roasters']\n",
    "\n",
    "# Separate countries with more than 1 roaster and those with exactly 1 roaster\n",
    "multiple_roasters = roasters_per_country[roasters_per_country['Number of Roasters'] > 1]\n",
    "single_roasters = roasters_per_country[roasters_per_country['Number of Roasters'] == 1]\n",
    "\n",
    "# Sum the single-roaster countries and create an \"Other Countries\" row\n",
    "other_countries_count = single_roasters['Number of Roasters'].sum()\n",
    "other_countries_row = pd.DataFrame({'Country': ['Other Countries \\n (Appearing only once)'], 'Number of Roasters': [other_countries_count]})\n",
    "\n",
    "# Combine the multiple_roasters and other_countries_row DataFrames\n",
    "final_roasters_per_country = pd.concat([multiple_roasters, other_countries_row], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Total number of roasters:\", total_roasters)\n",
    "print(\"\\nNumber of roasters per country (with 'Other Countries' grouped):\")\n",
    "display(final_roasters_per_country)\n",
    "\n",
    "# Plotting the number of roasters per country\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=final_roasters_per_country, y=\"Country\", x=\"Number of Roasters\")\n",
    "plt.title(\"Number of Roasters per Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. *(cont.)* Next, let's explore `products_beans_origins` and `roasters_countries` to see if they're redundant or if they present different data. Run the code below in a Python cell to display the `products_beans_origins` entries grouped by their `products_beans_origins` and `roasters_countries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables from the database\n",
    "products_beans = pd.read_sql(\"SELECT * FROM products_beans\", engine)\n",
    "roasters = pd.read_sql(\"SELECT * FROM roasters\", engine)\n",
    "roasters_countries = pd.read_sql(\"SELECT * FROM roasters_countries\", engine)\n",
    "beans_origins = pd.read_sql(\"SELECT * FROM products_beans_origins\", engine)\n",
    "\n",
    "# Merge the beans with origin and roaster country data\n",
    "beans_with_origin_country = pd.merge(products_beans, beans_origins, on=\"origin_id\", how=\"left\")\n",
    "beans_with_origin_country = pd.merge(beans_with_origin_country, roasters, on=\"roaster_id\", how=\"left\")\n",
    "beans_with_origin_country = pd.merge(beans_with_origin_country, roasters_countries, on=\"country_id\", how=\"left\")\n",
    "\n",
    "# Scenario 1: Coffee Beans with Similar Origins but Different Roaster Countries\n",
    "\n",
    "# Group by 'origin' and 'roaster_country' and count the products\n",
    "origin_country_summary = beans_with_origin_country.groupby(['origin', 'roaster_country']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter for origins that have multiple roaster countries\n",
    "origins_with_multiple_roasters = origin_country_summary.groupby('origin').filter(lambda x: x['roaster_country'].nunique() > 1)\n",
    "\n",
    "# Plotting Scenario 1\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=origins_with_multiple_roasters, x=\"origin\", y=\"Count\", hue=\"roaster_country\")\n",
    "plt.title(\"Coffee Beans with Similar Origins but Different Roaster Countries\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Scenario 2: Coffee Beans with Similar Roaster Countries but Different Origins\n",
    "\n",
    "# Group by 'roaster_country' and 'origin' and count the products\n",
    "country_origin_summary = beans_with_origin_country.groupby(['roaster_country', 'origin']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter for roaster countries that have multiple origins\n",
    "roasters_with_multiple_origins = country_origin_summary.groupby('roaster_country').filter(lambda x: x['origin'].nunique() > 1)\n",
    "\n",
    "# Plotting Scenario 2\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=roasters_with_multiple_origins, x=\"roaster_country\", y=\"Count\", hue=\"origin\", legend=False)\n",
    "plt.title(\"Coffee Beans with Similar Roaster Countries but Different Origins\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "8. Another table with data that will serve useful to the project is the `product_bean_reviews` Rather than being referenced by `product_beans` like our previous two tables of interset, `product_beans_reviews` references `product_beans`. Logically, this may imply that there are `product_beans_reviews` rows referring to the same `product_beans` entry. Although longer than the previous examples of categorical data, the \"description\" field containing multi-line entries of the flavor profiles of the specific `product_beans` rows they refer to.\n",
    "\n",
    "    Run the code cell below to visualize the char_count, word_count, mean_word_length, and mean_sent_length of the \"description\" column's entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the table name\n",
    "table_name = \"products_beans_reviews\"\n",
    "\n",
    "# Load the table's data into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Token count using BERT tokenizer (estimate)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with a suitable tokenizer if needed\n",
    "df['token_count'] = df['description'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Character count (already defined in your previous code)\n",
    "df['char_count'] = df['description'].str.len()\n",
    "\n",
    "print(df.head())  # View the first few rows to see the 'token_count' column\n",
    "\n",
    "tokens = tokenizer.tokenize(df['description'].iloc[0])\n",
    "\n",
    "print(\"\\n Index 0 Tokens:\", tokens)\n",
    "\n",
    "# Visualizations\n",
    "features = ['char_count', 'token_count']\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=df, y=feature)\n",
    "    plt.ylabel(feature)\n",
    "    plt.title(f'{feature}')\n",
    "\n",
    "    # KDE Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(data=df, x=feature)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER sentiment analysis on the description column\n",
    "df['sentiment_score'] = df['description'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Plotting sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['sentiment_score'], bins=30, kde=True)\n",
    "plt.title(\"Sentiment Score Distribution\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "9. Going back to the `product_beans` table we can first observe the distribution of the `roasts`. From our preliminary inspection we can see that the `roasts` table contains for entries, namely: Medium-Light, Medium, Light, Medium-Dark, Dark. This makes it an example of ordinal data. \n",
    "\n",
    "- This can easily be done using the Pandas and Seaborn Python libraries imported earlier. Run the code to produce a graph of `roasts` distribution: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the `product_beans` table into a DataFrame\n",
    "table_name_beans = \"products_beans\"\n",
    "query_beans = f\"SELECT * FROM {table_name_beans}\"\n",
    "product_beans_df = pd.read_sql(query_beans, engine)\n",
    "\n",
    "# Load the `roasts` table into a DataFrame\n",
    "table_name_roasts = \"roasts\"\n",
    "query_roasts = f\"SELECT * FROM {table_name_roasts}\"\n",
    "roasts_df = pd.read_sql(query_roasts, engine)\n",
    "\n",
    "# Merge the DataFrames on `roast_id`\n",
    "merged_df = pd.merge(product_beans_df, roasts_df, on=\"roast_id\", how=\"left\")\n",
    "\n",
    "# Count occurrences of each roast type in `products_beans`\n",
    "roast_distribution = merged_df['roast'].value_counts().reset_index()\n",
    "roast_distribution.columns = ['Roast Type', 'Count']\n",
    "\n",
    "# Plot the distribution as a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=roast_distribution, x='Roast Type', y='Count', palette=\"viridis\")\n",
    "plt.title(\"Distribution of Roasts in Product Beans\")\n",
    "plt.xlabel(\"Roast Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now let's tackle the interval (numerical) variables in our data. In this case it is the ratings contained the the ratings found in the `product_bean_reviews` table. We can plot a box plot to get an overview of the distribution of the ratings as well as any outliers that may need to be taken note of.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the table name\n",
    "table_name = \"products_beans_reviews\"\n",
    "\n",
    "# Load the table's data into a DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Display boxplots of ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(df['rating'])\n",
    "plt.title(\"Boxplot of Ratings\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Perform exploratory data analysis on the .csv file provided\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "11. First load the .csv file provided into a Python dataframe and display the head of the dataset. Run the Python code cell below in order to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('../learning-files/coffeepro-online-resources-exported.csv')\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "12. Run the code cell below to get an overview of the Products present in the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common products\n",
    "common_products = df_csv['Product'].value_counts()\n",
    "\n",
    "# Bar plot for top products\n",
    "plt.figure(figsize=(10, 6))\n",
    "common_products.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Coffee Products Featured')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Run the code cell below to get an overview of the \"Type\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the distribution of content types\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df_csv, x='Type')\n",
    "plt.title('Distribution of Content Types (Video vs Article)')\n",
    "plt.xlabel('Type of Content')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "14. Content Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_csv = pd.read_csv('../learning-files/coffeepro-online-resources-exported.csv')\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and calculate token counts and character counts for 'Content Focus' and 'Content Summary'\n",
    "for column in ['Content Focus']:\n",
    "    # Token count using BERT tokenizer\n",
    "    df_csv[f'{column}_token_count'] = df_csv[column].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "    # Character count\n",
    "    df_csv[f'{column}_char_count'] = df_csv[column].str.len()\n",
    "\n",
    "# Display first few rows to confirm the addition of new columns\n",
    "print(df_csv.head())\n",
    "\n",
    "# Visualize the character and token counts\n",
    "for feature in ['Content Focus_token_count', 'Content Focus_char_count']:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=df_csv, y=feature)\n",
    "    plt.ylabel(feature)\n",
    "    plt.title(f'{feature} Boxplot')\n",
    "\n",
    "    # KDE Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(data=df_csv, x=feature)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "15. Content Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_csv = pd.read_csv('../learning-files/coffeepro-online-resources-exported.csv')\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and calculate token counts and character counts for 'Content Focus' and 'Content Summary'\n",
    "for column in ['Content Summary']:\n",
    "    # Token count using BERT tokenizer\n",
    "    df_csv[f'{column}_token_count'] = df_csv[column].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "    # Character count\n",
    "    df_csv[f'{column}_char_count'] = df_csv[column].str.len()\n",
    "\n",
    "# Display first few rows to confirm the addition of new columns\n",
    "print(df_csv.head())\n",
    "\n",
    "# Visualize the character and token counts\n",
    "for feature in ['Content Summary_token_count', 'Content Summary_char_count']:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=df_csv, y=feature)\n",
    "    plt.ylabel(feature)\n",
    "    plt.title(f'{feature} Boxplot')\n",
    "\n",
    "    # KDE Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(data=df_csv, x=feature)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
