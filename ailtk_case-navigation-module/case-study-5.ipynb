{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Case Study: Evaluate models on use cases and for safety\n",
    "##### (GenAI Life Cycle Phase 5: Evaluation self-assesment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26520265d6e49069518f6e4880b5034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='<h3 style=\\'color: #1e7e34;\\'>Solution of \"(4) Case Study: Accessing cloud-based LLM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Cloud-Based LLM and RAG Content\n",
    "cloud_llm_rag_data = [\n",
    "    [\n",
    "        \"<b>(4a) Prompt Engineering in Google AI Studio:</b>\",\n",
    "        (\n",
    "            \"\"\"\n",
    "            <div>\n",
    "                <p>You should have designed and refined your own prompt in Google AI Studio to ensure the Gemini model \n",
    "                accurately interprets user intents and generates contextually relevant responses.</p>\n",
    "                <p>Below is a code segment you can use with a prompt usable for the given case:</p>\n",
    "                <div style='border: 1px dashed #1e7e34; padding: 10px; margin-top: 10px;'>\n",
    "                    <b>Code Segment:</b>\n",
    "                    <pre style='background-color: #f8f9fa; border: 1px solid #ccc; padding: 10px; font-family: monospace;'>\n",
    "# Import Google GenerativeAI Python module\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Define Gemini API key\n",
    "genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Specify model name and define system instruction\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\\\"\\\"\\\"You are a virtual restaurant and bar recommendation assistant. Your goal is to provide users with highly personalized recommendations based on their preferences and needs.\n",
    "\n",
    "Here are some guidelines to follow:\n",
    "\n",
    "- Understand the User's Intent: Carefully analyze the user's query.\n",
    "- Leverage User Preferences: Utilize the user's past behavior.\n",
    "- Consider Dietary Restrictions: Factor in dietary restrictions.\n",
    "- Provide Relevant Information: Offer details like cuisine type, price, and ambiance.\n",
    "- Handle Ambiguous Queries: Ask clarifying questions.\n",
    "- Be Conversational and Engaging: Maintain a friendly tone.\\\"\\\"\\\"\n",
    ")\n",
    "\n",
    "# Acceptable past chat for reference\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\"role\": \"user\", \"parts\": [\"Hello\"]},\n",
    "    {\"role\": \"model\", \"parts\": [\"Hello there! I am a virtual agent for Welp!\"]},\n",
    "  ]\n",
    ")\n",
    "                    </pre>\n",
    "                </div>\n",
    "                <div style='margin-top: 10px;'>\n",
    "                    <a href='case-files/ailtk-running-code-case-3.ipynb' target='_blank' style='color: #1e7e34; text-decoration: underline;'>Click here to open Solution: Case Study 4 in Visual Studio Code</a>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "    ],\n",
    "    [\n",
    "        \"<b>(4b) API Integration:</b>\",\n",
    "        (\n",
    "            \"You will be able to reuse your API key from Practice Learning Activity 4 for the sake of this case.<br>\"\n",
    "            \"<div style='margin-top: 10px;'>\"\n",
    "            \"<a href='#' target='_blank' style='color: #1e7e34; text-decoration: underline;'>Click here to review receiving and accessing your API key</a>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "    ],\n",
    "    [\n",
    "        \"<b>(4c) Data Utilization with RAG:</b>\",\n",
    "        (\n",
    "            \"Apply Retrieval-Augmented Generation (RAG) techniques to combine the fine-tuned Gemini model with curated datasets.<br>\"\n",
    "            \"<div style='border: 1px dashed #1e7e34; padding: 10px; margin-top: 10px;'>\"\n",
    "            \"<b>Code Segment:</b><br><pre style='background-color: #f8f9fa; border: 1px solid #ccc; padding: 10px; font-family: monospace;'>\"\n",
    "            \"\"\"\n",
    "# Example: Jaccard Similarity for Document Matching\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"solution-case-study-activity-3/ailtk-case-apache-hop-output.xls\")\n",
    "\n",
    "corpus = df.apply(lambda row: f\"{row['input']}. {row['output']}\", axis=1).tolist()\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def return_response(query, corpus, top_n=5):\n",
    "    similarities = []\n",
    "    \n",
    "    # Calculate similarity for each document in the corpus\n",
    "    for doc in corpus:\n",
    "        similarity = jaccard_similarity(query, doc)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Get the indices of the top_n most similar documents\n",
    "    top_n_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_n]\n",
    "    \n",
    "    # Return the top_n most similar documents\n",
    "    top_n_documents = [corpus[i] for i in top_n_indices]\n",
    "    \n",
    "    return top_n_documents\n",
    "\n",
    "# Define a function to find documents similar to the user's input, \n",
    "# Provide LLM with an injected prompt, and receive response\n",
    "\n",
    "def generate_response_with_injected_prompt(user_prompt, corpus, model):\n",
    "# Generates a response using a model with injected prompt from RAG results.\n",
    "\n",
    "# Parameters:\n",
    "# - user_prompt (str): The user's input prompt (e.g., preferences for coffee).\n",
    "# - corpus (list): The corpus of documents to search for similarities.\n",
    "# - model (object): The model used to generate content based on the injected prompt.\n",
    "    \n",
    "    # RAG result on the user's input\n",
    "    rag_result = return_response(user_prompt, corpus)\n",
    "    \n",
    "    # View five most similar documents from corpus according to jaccard similarity\n",
    "    print(rag_result)\n",
    "    \n",
    "    # Append input to create an injected prompt\n",
    "    injected_prompt = f\"{user_prompt} {rag_result}\"\n",
    "    \n",
    "    # Call your model and input the injected prompt\n",
    "    response = model.generate_content(injected_prompt)\n",
    "    \n",
    "    # Return the response text\n",
    "    return response.text\n",
    "\n",
    "            \"\"\"\n",
    "            \"</pre></div>\"\n",
    "        )\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create content for the widget\n",
    "cloud_llm_rag_content = widgets.VBox([widgets.HTML(value=f\"{item[0]}<br>{item[1]}\") for item in cloud_llm_rag_data])\n",
    "\n",
    "# Styled Box for Cloud-Based LLM and RAG\n",
    "styled_cloud_llm_box = widgets.Box(\n",
    "    [\n",
    "        widgets.HTML(\n",
    "            value=\"<h3 style='color: #1e7e34;'>Solution of \\\"(4) Case Study: Accessing cloud-based LLM models and implementing RAG\\\"</h3>\"\n",
    "        ),\n",
    "        widgets.HTML(value=\"<hr style='border: 1px solid #1e7e34;'>\"),  # Horizontal line for separation\n",
    "        cloud_llm_rag_content,\n",
    "    ],\n",
    "    layout=widgets.Layout(\n",
    "        border=\"2px solid #1e7e34\",\n",
    "        padding=\"20px\",\n",
    "        width=\"90%\",\n",
    "        margin=\"20px 0px\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the styled box\n",
    "display(styled_cloud_llm_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Scenario\n",
    "> Welp's restaurant recommendation virtual assistant has now progressed to the evaluation phase, where its capabilities and safety must be thoroughly tested to ensure it meets real-world demands and adheres to ethical AI guidelines. With its Retrieval-Augmented Generation (RAG) system integrated, the assistant can provide personalized restaurant suggestions and contextual justifications. However, it is essential to confirm that these recommendations are accurate, inclusive, and aligned with user expectations.\n",
    ">\n",
    "> As the AI developer, your task is to evaluate the assistant’s performance across various use cases. This includes testing its ability to handle diverse user inputs—ranging from vague or incomplete queries to specific and detailed requests. Additionally, you must assess the assistant's adherence to ethical AI principles, such as avoiding biased or discriminatory suggestions, and ensuring that its responses remain neutral and user-focused.\n",
    ">\n",
    "> The virtual assistant must also maintain a professional and approachable tone, consistent with Welp's branding, to instill trust and encourage user engagement.\n",
    ">\n",
    "> Your Tasks:\n",
    "> \n",
    "> (a) Performance Evaluation:\n",
    "> Test the virtual assistant’s accuracy and relevance in delivering restaurant recommendations across multiple scenarios, including users with dietary restrictions, specific cuisine preferences, or location constraints. Analyze edge cases, such as conflicting user inputs or ambiguous requests.\n",
    ">\n",
    ">(b) Safety and Ethical Testing:\n",
    "> Examine the assistant’s outputs for potential biases or safety concerns. For instance, verify that the assistant does not promote unhealthy eating habits or unfairly prioritize certain restaurant categories over others. Additionally, ensure that its recommendations remain respectful and appropriate for diverse cultural contexts.\n",
    "> \n",
    "> By the end of this activity, you will have applied best practices for model evaluation, gaining hands-on experience in ensuring that AI systems are not only functional but also safe, ethical, and aligned with user needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-requisite:\n",
    "- Create a Jupyter Notebook and load your code for RAG (from the previous Chapter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the tasks as follows:\n",
    "\n",
    "\n",
    "#### **(a) Performance Evaluation:** Test the virtual assistant’s accuracy and relevance in delivering restaurant recommendations across multiple scenarios, including users with dietary restrictions, specific cuisine preferences, or location constraints.\n",
    "\n",
    "#### **(b) Safety and Ethical Testing:** Examine the assistant’s outputs for potential biases or safety concerns.\n",
    "\n",
    "<a href='https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=trending' target='_blank'>Find use cases here</a>\n",
    "\n",
    "<a href='https://huggingface.co/collections/harpreetsahota/red-teaming-prompts-656256235475849b82a91813' target='_blank'>And here</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer the following to proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f622ac70c042cb9010f05df5c520ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q1: What is the primary goal of performance testing?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524f1947f7c14201a9c902ca33ef8415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('To identify edge cases and test model robust…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6d1bad6e3e4b65a3bb4ee750710bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q2: Which input should the model evaluate for possible ethical problems?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5df3959f4d457dbb06e2fbc5a3cbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Common user preferences', 'Beginner coffee d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f64edfe7174d3ba861e8236d1316ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q3: What is a key task in the safety and ethical review of virtual agents?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184e239b3b724628b02cb41460c0e974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Testing the agent for biases in recommendati…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addd9c1eed284ef9b8cd7086430fedfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q4: What should the virtual agent avoid to maintain brand guidelines?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb5e4117ab64e0fbe6d7892d4e11375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Using an overly casual tone', 'Suggesting ne…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7141768e308474996dc14d81dd1b8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q5: What tool is suggested for identifying potential vulnerabilities in the AI system?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cfdea99c454648bd6270ce5e3e814f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Red teaming', 'Data augmentation', 'Performa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13af9d2bfb464c37875ea3c8ca7ba9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q6: What is the expected behavior of the agent when handling ambiguous inputs?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c88362966a64624842eef70e578dfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('It should ask clarifying questions.', 'It sh…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f5c473dc71486c83cdaa0f9e2f1e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q7: What criteria should the model meet to pass safety evaluations?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532187bbc3e64fa48ff1e024c1f267bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Accurate, unbiased, and practical recommenda…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d650d8be54cc401eb610450de2c22092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q8: What is the purpose of red team prompts in model evaluation?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498958c2b85a4b5685d6f8e70081f86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('To identify flaws in the model’s ethical or …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1af94f36254582b6d3e061345c9072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q9: How does the system handle user input that requests unethical actions?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1f2126ebda42c7ab26a41ae4dd4a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('It denies the request and provides ethical g…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9806681ed64985afdb18532ef546af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q10: What is an example of an edge case a model must be evaluated on?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa61c460b5e2438c9e3ad012fb531335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Handling contradictory or ambiguous user inp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418dbbb21889491891e8f2f6dfaa49a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit Answers', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726fa51fd4cb483e8eba874ab41b5b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Define questions and options\n",
    "questions = [\n",
    "    {\n",
    "        \"question\": \"What is the primary goal of performance testing?\",\n",
    "        \"options\": [\n",
    "            \"To identify edge cases and test model robustness\",\n",
    "            \"To ensure the agent adheres to ethical AI guidelines\",\n",
    "            \"To simulate scenarios and ensure accurate recommendations\",\n",
    "            \"To train the model on additional industry-specific data\"\n",
    "        ],\n",
    "        \"answer\": \"To simulate scenarios and ensure accurate recommendations\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which input should the model evaluate for possible ethical problems?\",\n",
    "        \"options\": [\n",
    "            \"Common user preferences\",\n",
    "            \"Beginner coffee drinker queries\",\n",
    "            \"Advanced enthusiast requests\",\n",
    "            \"\\\"Red team\\\" prompts and edge cases\"\n",
    "        ],\n",
    "        \"answer\": \"\\\"Red team\\\" prompts and edge cases\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a key task in the safety and ethical review of virtual agents?\",\n",
    "        \"options\": [\n",
    "            \"Testing the agent for biases in recommendations\",\n",
    "            \"Adding new features for advanced enthusiasts\",\n",
    "            \"Optimizing the agent's response time\",\n",
    "            \"Reducing the corpus size for faster processing\"\n",
    "        ],\n",
    "        \"answer\": \"Testing the agent for biases in recommendations\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What should the virtual agent avoid to maintain brand guidelines?\",\n",
    "        \"options\": [\n",
    "            \"Using an overly casual tone\",\n",
    "            \"Suggesting new possible things to try\",\n",
    "            \"Providing info from the corpus provided\",\n",
    "            \"Recommending unique or less popular options\"\n",
    "        ],\n",
    "        \"answer\": \"Using an overly casual tone\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tool is suggested for identifying potential vulnerabilities in the AI system?\",\n",
    "        \"options\": [\n",
    "            \"Red teaming\",\n",
    "            \"Data augmentation\",\n",
    "            \"Performance tuning\",\n",
    "            \"Corpus filtering\"\n",
    "        ],\n",
    "        \"answer\": \"Red teaming\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the expected behavior of the agent when handling ambiguous inputs?\",\n",
    "        \"options\": [\n",
    "            \"It should ask clarifying questions.\",\n",
    "            \"It should recommend the most popular coffee.\",\n",
    "            \"It should generate a random coffee suggestion.\",\n",
    "            \"It should refer the user to customer support.\"\n",
    "        ],\n",
    "        \"answer\": \"It should ask clarifying questions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What criteria should the model meet to pass safety evaluations?\",\n",
    "        \"options\": [\n",
    "            \"Accurate, unbiased, and practical recommendations\",\n",
    "            \"Speed of response under 0.5 seconds\",\n",
    "            \"Support for all global languages\",\n",
    "            \"Ability to generate a complete coffee brewing guide\"\n",
    "        ],\n",
    "        \"answer\": \"Accurate, unbiased, and practical recommendations\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the purpose of red team prompts in model evaluation?\",\n",
    "        \"options\": [\n",
    "            \"To identify flaws in the model’s ethical or safety handling\",\n",
    "            \"To provide additional training data for the model\",\n",
    "            \"To improve the speed of the recommendation engine\",\n",
    "            \"To add variety to user queries\"\n",
    "        ],\n",
    "        \"answer\": \"To identify flaws in the model’s ethical or safety handling\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the system handle user input that requests unethical actions?\",\n",
    "        \"options\": [\n",
    "            \"It denies the request and provides ethical guidelines.\",\n",
    "            \"It generates a placeholder response.\",\n",
    "            \"It logs the input for future training.\",\n",
    "            \"It refers the query to a human operator.\"\n",
    "        ],\n",
    "        \"answer\": \"It denies the request and provides ethical guidelines.\"\n",
    "    },\n",
    "    {\n",
    "    \"question\": \"What is an example of an edge case a model must be evaluated on?\",\n",
    "    \"options\": [\n",
    "        \"Handling contradictory or ambiguous user inputs\",\n",
    "        \"Providing information on commonly known topics\",\n",
    "        \"Responding to straightforward, factual queries\",\n",
    "        \"Delivering recommendations within well-defined parameters\"\n",
    "    ],\n",
    "    \"answer\": \"Handling contradictory or ambiguous user inputs\"\n",
    "}\n",
    "]\n",
    "\n",
    "# Widgets for questions\n",
    "quiz_widgets = []\n",
    "for i, q in enumerate(questions):\n",
    "    question_label = widgets.Label(value=f\"Q{i+1}: {q['question']}\")\n",
    "    options = widgets.RadioButtons(\n",
    "        options=q['options'],\n",
    "        description='',\n",
    "        disabled=False,\n",
    "        value=None,\n",
    "        layout=widgets.Layout(width='90%', height='auto')  # Ensures proper layout for longer options\n",
    "    )\n",
    "    quiz_widgets.append((question_label, options))\n",
    "\n",
    "# Button to submit answers\n",
    "submit_button = widgets.Button(description=\"Submit Answers\", button_style=\"primary\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Flag to track if the error message is already displayed\n",
    "error_displayed = False\n",
    "\n",
    "# Define button click event\n",
    "def on_submit_click(b):\n",
    "    global error_displayed\n",
    "    # Disable the submit button\n",
    "    submit_button.disabled = True\n",
    "    clear_output(wait=True)\n",
    "    unanswered = False\n",
    "    score = 0\n",
    "\n",
    "    # Check if all questions are answered\n",
    "    for i, (label, options) in enumerate(quiz_widgets):\n",
    "        if options.value is None:  # If a question is left unanswered\n",
    "            unanswered = True\n",
    "\n",
    "    with output:\n",
    "        if unanswered:\n",
    "            if not error_displayed:  # Only display the error if it hasn't been shown already\n",
    "                error_displayed = True\n",
    "                # Display error message in red\n",
    "                display(widgets.HTML(\n",
    "                    '<p style=\"color: red; font-weight: bold;\">Please answer all the questions before submitting.</p>'\n",
    "                ))\n",
    "            submit_button.disabled = False  # Re-enable button if there's an error\n",
    "        else:\n",
    "            error_displayed = False  # Reset the flag if all questions are answered\n",
    "            submit_button.button_style = \"\"  # Reset button style to default after click\n",
    "            # Calculate score\n",
    "            for i, (label, options) in enumerate(quiz_widgets):\n",
    "                user_answer = options.value\n",
    "                correct_answer = questions[i][\"answer\"]\n",
    "                if user_answer == correct_answer:\n",
    "                    score += 1\n",
    "                print(f\"Q{i+1}: {questions[i]['question']}\")\n",
    "                print(f\"  - Your answer: {user_answer}\")\n",
    "                print(f\"  - Correct answer: {correct_answer}\")\n",
    "                print()\n",
    "\n",
    "            print(f\"You scored {score}/{len(questions)}! ({(score / len(questions)) * 100:.2f}%)\")\n",
    "            \n",
    "            # Show Continue or Try Again button based on score\n",
    "            if score >= 0.8 * len(questions):\n",
    "                continue_button = widgets.HTML(\n",
    "                    '<a href=\"case-study-4.ipynb\" style=\"display: inline-block; padding: 10px 15px; '\n",
    "                    'background-color: #28a745; color: white; text-decoration: none; border-radius: 5px;\">'\n",
    "                    'Continue</a>'\n",
    "                )\n",
    "                display(continue_button)\n",
    "            else:\n",
    "                try_again_button = widgets.HTML(\n",
    "                    '<a href=\"case-study-3.ipynb\" style=\"display: inline-block; padding: 10px 15px; '\n",
    "                    'background-color: #dc3545; color: white; text-decoration: none; border-radius: 5px;\">'\n",
    "                    'Score at least 80% to continue. Try Again</a>'\n",
    "                )\n",
    "                display(try_again_button)\n",
    "\n",
    "# Attach event to the submit button\n",
    "submit_button.on_click(on_submit_click)\n",
    "\n",
    "# Display the quiz\n",
    "for label, options in quiz_widgets:\n",
    "    display(label, options)\n",
    "display(submit_button, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***END CAP1*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[***TODO CAP2 - Proceed to Case Study***] Next: Case Study 6](case-study-6.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
