{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Case Study: Evaluate models on use cases and for safety\n",
    "##### (GenAI Life Cycle Phase 5: Evaluation self-assesment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807e43f8cd0b4b8388dd91c5b62db0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='<h3 style=\\'color: #1e7e34;\\'>PRE-READING: Solution of \"(4) Case Study: Accessing clâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Cloud-Based LLM and RAG Content\n",
    "cloud_llm_rag_data = [\n",
    "    [\n",
    "        \"<b>(4a) Prompt Engineering in Google AI Studio:</b>\",\n",
    "        (\n",
    "            \"\"\"\n",
    "            <div>\n",
    "                <p>You should have designed and refined your own prompt in Google AI Studio to ensure the Gemini model \n",
    "                accurately interprets user intents and generates contextually relevant responses.</p>\n",
    "                <p>Below is a code segment you can use with a prompt usable for the given case:</p>\n",
    "                <div style='border: 1px dashed #1e7e34; padding: 10px; margin-top: 10px;'>\n",
    "                    <b>Code Segment:</b>\n",
    "                    <pre style='background-color: #f8f9fa; border: 1px solid #ccc; padding: 10px; font-family: monospace;'>\n",
    "# Import Google GenerativeAI Python module\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Define Gemini API key\n",
    "genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Specify model name and define system instruction\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\\\"\\\"\\\"You are a virtual restaurant and bar recommendation assistant. Your goal is to provide users with highly personalized recommendations based on their preferences and needs.\n",
    "\n",
    "Here are some guidelines to follow:\n",
    "\n",
    "- Understand the User's Intent: Carefully analyze the user's query.\n",
    "- Leverage User Preferences: Utilize the user's past behavior.\n",
    "- Consider Dietary Restrictions: Factor in dietary restrictions.\n",
    "- Provide Relevant Information: Offer details like cuisine type, price, and ambiance.\n",
    "- Handle Ambiguous Queries: Ask clarifying questions.\n",
    "- Be Conversational and Engaging: Maintain a friendly tone.\\\"\\\"\\\"\n",
    ")\n",
    "\n",
    "# Acceptable past chat for reference\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\"role\": \"user\", \"parts\": [\"Hello\"]},\n",
    "    {\"role\": \"model\", \"parts\": [\"Hello there! I am a virtual agent for Welp!\"]},\n",
    "  ]\n",
    ")\n",
    "                    </pre>\n",
    "                </div>\n",
    "                <div style='margin-top: 10px;'>\n",
    "                    <a href='case-files/ailtk-running-code-case-4.ipynb' target='_blank' style='color: #1e7e34; text-decoration: underline;'>Click here to open Solution: Case Study 4 in Visual Studio Code</a>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "    ],\n",
    "    [\n",
    "        \"<b>(4b) API Integration:</b>\",\n",
    "        (\n",
    "            \"You will be able to reuse your API key from Practice Learning Activity 4 for the sake of this case.<br>\"\n",
    "            \"<div style='margin-top: 10px;'>\"\n",
    "            \"<a href='#' target='_blank' style='color: #1e7e34; text-decoration: underline;'>Click here to review receiving and accessing your API key</a>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "    ],\n",
    "    [\n",
    "        \"<b>(4c) Data Utilization with RAG:</b>\",\n",
    "        (\n",
    "            \"Apply Retrieval-Augmented Generation (RAG) techniques to combine the fine-tuned Gemini model with curated datasets.<br>\"\n",
    "            \"<div style='border: 1px dashed #1e7e34; padding: 10px; margin-top: 10px;'>\"\n",
    "            \"<b>Code Segment:</b><br><pre style='background-color: #f8f9fa; border: 1px solid #ccc; padding: 10px; font-family: monospace;'>\"\n",
    "            \"\"\"\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"solution-case-study-activity-3/ailtk-case-apache-hop-output.xls\")\n",
    "\n",
    "# Combine relevant columns into a single document per row\n",
    "# Assuming the columns are 'input' and 'output', adjust if necessary\n",
    "corpus = df.apply(lambda row: f\"{row['input']}. {row['output']}\", axis=1).tolist()\n",
    "\n",
    "# Save corpus to a pickle file\n",
    "PICKLE_FILE = \"corpus.pkl\"\n",
    "\n",
    "with open(PICKLE_FILE, \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "print(f\"Corpus successfully saved to {PICKLE_FILE}\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "class RAGOrchestrator:\n",
    "    # Manages corpus loading, similarity calculations, and generating augmented responses using the LLM.\n",
    "\n",
    "    def __init__(self, pickle_file: str, model):\n",
    "        # Initializes the RAGOrchestrator.\n",
    "        # Parameters:\n",
    "        # - pickle_file (str): Path to the pickled corpus file.\n",
    "        # - model: Preloaded LLM instance for generating responses.\n",
    "\n",
    "        self.pickle_file = pickle_file\n",
    "        self.model = model\n",
    "        self.corpus = self._load_corpus()\n",
    "\n",
    "    def _load_corpus(self) -> List[str]:\n",
    "        # Loads the corpus from a pickle file.\n",
    "        \n",
    "        if not os.path.exists(self.pickle_file):\n",
    "            raise FileNotFoundError(f\"Pickle file '{self.pickle_file}' not found. Please generate it first.\")\n",
    "        \n",
    "        with open(self.pickle_file, \"rb\") as f:\n",
    "            print(\"Corpus loaded from pickle file.\")\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def _jaccard_similarity(query: str, document: str) -> float:\n",
    "        # Calculates Jaccard similarity between a query and a document.\n",
    "        \n",
    "        query_tokens = set(query.lower().split())\n",
    "        document_tokens = set(document.lower().split())\n",
    "        \n",
    "        intersection = query_tokens.intersection(document_tokens)\n",
    "        union = query_tokens.union(document_tokens)\n",
    "\n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "    def _get_similar_documents(self, query: str, top_n: int = 5) -> List[str]:\n",
    "        # Retrieves the top N most similar documents from the corpus.\n",
    "        \n",
    "        similarities = [self._jaccard_similarity(query, doc) for doc in self.corpus]\n",
    "        top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_n]\n",
    "        \n",
    "        return [self.corpus[i] for i in top_indices]\n",
    "\n",
    "    def generate_augmented_response(self, user_prompt: str) -> str:\n",
    "        # Generates a response using the LLM with an injected prompt from RAG results.\n",
    "\n",
    "        similar_docs = self._get_similar_documents(user_prompt)\n",
    "        injected_prompt = f\"{user_prompt} {' '.join(similar_docs)}\"\n",
    "\n",
    "        response = self.model.generate_content(injected_prompt)\n",
    "        return response.text\n",
    "\n",
    "# Example usage:\n",
    "# PICKLE_FILE = \"corpus.pkl\"\n",
    "# MODEL = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "# orchestrator = RAGOrchestrator(PICKLE_FILE, MODEL)\n",
    "# response = orchestrator.generate_augmented_response(\"Tell me about coffee preparation methods.\")\n",
    "# print(response)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            \"</pre></div>\"\n",
    "        )\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create content for the widget\n",
    "cloud_llm_rag_content = widgets.VBox([widgets.HTML(value=f\"{item[0]}<br>{item[1]}\") for item in cloud_llm_rag_data])\n",
    "\n",
    "# Styled Box for Cloud-Based LLM and RAG\n",
    "styled_cloud_llm_box = widgets.Box(\n",
    "    [\n",
    "        widgets.HTML(\n",
    "            value=\"<h3 style='color: #1e7e34;'>PRE-READING: Solution of \\\"(4) Case Study: Accessing cloud-based LLM models and implementing RAG\\\"</h3>\"\n",
    "        ),\n",
    "        widgets.HTML(value=\"<hr style='border: 1px solid #1e7e34;'>\"),  # Horizontal line for separation\n",
    "        cloud_llm_rag_content,\n",
    "    ],\n",
    "    layout=widgets.Layout(\n",
    "        border=\"2px solid #1e7e34\",\n",
    "        padding=\"20px\",\n",
    "        width=\"90%\",\n",
    "        margin=\"20px 0px\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the styled box\n",
    "display(styled_cloud_llm_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Scenario\n",
    "> Welp's restaurant recommendation virtual assistant has now progressed to the evaluation phase, where its capabilities and safety must be thoroughly tested to ensure it meets real-world demands and adheres to ethical AI guidelines. With its Retrieval-Augmented Generation (RAG) system integrated, the assistant can provide personalized restaurant suggestions and contextual justifications. However, it is essential to confirm that these recommendations are accurate, inclusive, and aligned with user expectations.\n",
    ">\n",
    "> As the AI developer, your task is to evaluate the assistantâ€™s performance across various use cases. This includes testing its ability to handle diverse user inputsâ€”ranging from vague or incomplete queries to specific and detailed requests. Additionally, you must assess the assistant's adherence to ethical AI principles, such as avoiding biased or discriminatory suggestions, and ensuring that its responses remain neutral and user-focused.\n",
    ">\n",
    "> The virtual assistant must also maintain a professional and approachable tone, consistent with Welp's branding, to instill trust and encourage user engagement.\n",
    ">\n",
    "> Your Tasks:\n",
    "> \n",
    "> (a) Performance Evaluation:\n",
    "> Test the virtual assistantâ€™s accuracy and relevance in delivering restaurant recommendations across multiple scenarios, including users with dietary restrictions, specific cuisine preferences, or location constraints. Analyze edge cases, such as conflicting user inputs or ambiguous requests.\n",
    ">\n",
    ">(b) Safety and Ethical Testing:\n",
    "> Examine the assistantâ€™s outputs for potential biases or safety concerns. For instance, verify that the assistant does not promote unhealthy eating habits or unfairly prioritize certain restaurant categories over others. Additionally, ensure that its recommendations remain respectful and appropriate for diverse cultural contexts.\n",
    "> \n",
    "> By the end of this activity, you will have applied best practices for model evaluation, gaining hands-on experience in ensuring that AI systems are not only functional but also safe, ethical, and aligned with user needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite:\n",
    "- Create a Jupyter Notebook and load your code for RAG (from the previous Chapter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the tasks as follows:\n",
    "\n",
    "\n",
    "#### **(a) Performance Evaluation:** Test the virtual assistantâ€™s accuracy and relevance in delivering restaurant recommendations across multiple scenarios, including users with dietary restrictions, specific cuisine preferences, or location constraints.\n",
    "\n",
    "#### **(b) Safety and Ethical Testing:** Examine the assistantâ€™s outputs for potential biases or safety concerns.\n",
    "\n",
    "- <a href='https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=trending' target='_blank'>Find use cases here</a>\n",
    "\n",
    "- <a href='https://huggingface.co/collections/harpreetsahota/red-teaming-prompts-656256235475849b82a91813' target='_blank'>And here</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer the following to proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f92935402f4775acbf06d0cf87fd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q1: What is the primary goal of performance testing?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671d4c5462f243be9f848ee2a9715de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('To identify edge cases and test model robustâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532ef16c7f864f1ba1fb755e296469c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q2: Which input should the model evaluate for possible ethical problems?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abee7dd4def424f8f63ecfec3c73c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Common user preferences', 'Beginner coffee dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7b8cfb1fe644da9ce699d194438de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q3: What is a key task in the safety and ethical review of virtual agents?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6400d00297784e0b8020c4b3f874c240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Testing the agent for biases in recommendatiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43016700ddf452b97f9af78875464a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q4: What should the virtual agent avoid to maintain brand guidelines?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e578a6534fa4efd8f3ced71edd7a036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Using an overly casual tone', 'Suggesting neâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8a4596636d45c38b7ef3d704ba956b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q5: What tool is suggested for identifying potential vulnerabilities in the AI system?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e047ae9aa34f03bf6621d897b1f715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Red teaming', 'Data augmentation', 'Performaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb64e91d4dd9410588e7495322d65b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q6: What is the expected behavior of the agent when handling ambiguous inputs?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb10a96fca6407397d472cb594b5f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('It should ask clarifying questions.', 'It shâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f97f4170424a2c9556d63675c4773f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q7: What criteria should the model meet to pass safety evaluations?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955821def3c04b18a0f21e6705d6718c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Accurate, unbiased, and practical recommendaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd594417e64433f96a65edc7f74ff7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q8: What is the purpose of red team prompts in model evaluation?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c64e65c0b5486fa6f13e873f038a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('To identify flaws in the modelâ€™s ethical or â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed625611224f61806a7ae950851520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q9: How does the system handle user input that requests unethical actions?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c75b2bb57c34253b96192463ac891b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('It denies the request and provides ethical gâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ec62978f55422a90cde0258eda8217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Q10: What is an example of an edge case a model must be evaluated on?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41874ae046794d8384343ec46c56ba2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='auto', width='90%'), options=('Handling contradictory or ambiguous user inpâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db55a4933f34443d93b71dd65a5f2792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit Answers', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88af80d0dcc42fa80579b5ff4253826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Define questions and options\n",
    "questions = [\n",
    "    {\n",
    "        \"question\": \"What is the primary goal of performance testing?\",\n",
    "        \"options\": [\n",
    "            \"To identify edge cases and test model robustness\",\n",
    "            \"To ensure the agent adheres to ethical AI guidelines\",\n",
    "            \"To simulate scenarios and ensure accurate recommendations\",\n",
    "            \"To train the model on additional industry-specific data\"\n",
    "        ],\n",
    "        \"answer\": \"To simulate scenarios and ensure accurate recommendations\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which input should the model evaluate for possible ethical problems?\",\n",
    "        \"options\": [\n",
    "            \"Common user preferences\",\n",
    "            \"Beginner coffee drinker queries\",\n",
    "            \"Advanced enthusiast requests\",\n",
    "            \"\\\"Red team\\\" prompts and edge cases\"\n",
    "        ],\n",
    "        \"answer\": \"\\\"Red team\\\" prompts and edge cases\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a key task in the safety and ethical review of virtual agents?\",\n",
    "        \"options\": [\n",
    "            \"Testing the agent for biases in recommendations\",\n",
    "            \"Adding new features for advanced enthusiasts\",\n",
    "            \"Optimizing the agent's response time\",\n",
    "            \"Reducing the corpus size for faster processing\"\n",
    "        ],\n",
    "        \"answer\": \"Testing the agent for biases in recommendations\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What should the virtual agent avoid to maintain brand guidelines?\",\n",
    "        \"options\": [\n",
    "            \"Using an overly casual tone\",\n",
    "            \"Suggesting new possible things to try\",\n",
    "            \"Providing info from the corpus provided\",\n",
    "            \"Recommending unique or less popular options\"\n",
    "        ],\n",
    "        \"answer\": \"Using an overly casual tone\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tool is suggested for identifying potential vulnerabilities in the AI system?\",\n",
    "        \"options\": [\n",
    "            \"Red teaming\",\n",
    "            \"Data augmentation\",\n",
    "            \"Performance tuning\",\n",
    "            \"Corpus filtering\"\n",
    "        ],\n",
    "        \"answer\": \"Red teaming\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the expected behavior of the agent when handling ambiguous inputs?\",\n",
    "        \"options\": [\n",
    "            \"It should ask clarifying questions.\",\n",
    "            \"It should recommend the most popular coffee.\",\n",
    "            \"It should generate a random coffee suggestion.\",\n",
    "            \"It should refer the user to customer support.\"\n",
    "        ],\n",
    "        \"answer\": \"It should ask clarifying questions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What criteria should the model meet to pass safety evaluations?\",\n",
    "        \"options\": [\n",
    "            \"Accurate, unbiased, and practical recommendations\",\n",
    "            \"Speed of response under 0.5 seconds\",\n",
    "            \"Support for all global languages\",\n",
    "            \"Ability to generate a complete coffee brewing guide\"\n",
    "        ],\n",
    "        \"answer\": \"Accurate, unbiased, and practical recommendations\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the purpose of red team prompts in model evaluation?\",\n",
    "        \"options\": [\n",
    "            \"To identify flaws in the modelâ€™s ethical or safety handling\",\n",
    "            \"To provide additional training data for the model\",\n",
    "            \"To improve the speed of the recommendation engine\",\n",
    "            \"To add variety to user queries\"\n",
    "        ],\n",
    "        \"answer\": \"To identify flaws in the modelâ€™s ethical or safety handling\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the system handle user input that requests unethical actions?\",\n",
    "        \"options\": [\n",
    "            \"It denies the request and provides ethical guidelines.\",\n",
    "            \"It generates a placeholder response.\",\n",
    "            \"It logs the input for future training.\",\n",
    "            \"It refers the query to a human operator.\"\n",
    "        ],\n",
    "        \"answer\": \"It denies the request and provides ethical guidelines.\"\n",
    "    },\n",
    "    {\n",
    "    \"question\": \"What is an example of an edge case a model must be evaluated on?\",\n",
    "    \"options\": [\n",
    "        \"Handling contradictory or ambiguous user inputs\",\n",
    "        \"Providing information on commonly known topics\",\n",
    "        \"Responding to straightforward, factual queries\",\n",
    "        \"Delivering recommendations within well-defined parameters\"\n",
    "    ],\n",
    "    \"answer\": \"Handling contradictory or ambiguous user inputs\"\n",
    "}\n",
    "]\n",
    "\n",
    "# Widgets for questions\n",
    "quiz_widgets = []\n",
    "for i, q in enumerate(questions):\n",
    "    question_label = widgets.Label(value=f\"Q{i+1}: {q['question']}\")\n",
    "    options = widgets.RadioButtons(\n",
    "        options=q['options'],\n",
    "        description='',\n",
    "        disabled=False,\n",
    "        value=None,\n",
    "        layout=widgets.Layout(width='90%', height='auto')  # Ensures proper layout for longer options\n",
    "    )\n",
    "    quiz_widgets.append((question_label, options))\n",
    "\n",
    "# Button to submit answers\n",
    "submit_button = widgets.Button(description=\"Submit Answers\", button_style=\"primary\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Flag to track if the error message is already displayed\n",
    "error_displayed = False\n",
    "\n",
    "# Define button click event\n",
    "def on_submit_click(b):\n",
    "    global error_displayed\n",
    "    # Disable the submit button\n",
    "    submit_button.disabled = True\n",
    "    clear_output(wait=True)\n",
    "    unanswered = False\n",
    "    score = 0\n",
    "\n",
    "    # Check if all questions are answered\n",
    "    for i, (label, options) in enumerate(quiz_widgets):\n",
    "        if options.value is None:  # If a question is left unanswered\n",
    "            unanswered = True\n",
    "\n",
    "    with output:\n",
    "        if unanswered:\n",
    "            if not error_displayed:  # Only display the error if it hasn't been shown already\n",
    "                error_displayed = True\n",
    "                # Display error message in red\n",
    "                display(widgets.HTML(\n",
    "                    '<p style=\"color: red; font-weight: bold;\">Please answer all the questions before submitting.</p>'\n",
    "                ))\n",
    "            submit_button.disabled = False  # Re-enable button if there's an error\n",
    "        else:\n",
    "            error_displayed = False  # Reset the flag if all questions are answered\n",
    "            submit_button.button_style = \"\"  # Reset button style to default after click\n",
    "            # Calculate score\n",
    "            for i, (label, options) in enumerate(quiz_widgets):\n",
    "                user_answer = options.value\n",
    "                correct_answer = questions[i][\"answer\"]\n",
    "                if user_answer == correct_answer:\n",
    "                    score += 1\n",
    "                print(f\"Q{i+1}: {questions[i]['question']}\")\n",
    "                print(f\"  - Your answer: {user_answer}\")\n",
    "                print(f\"  - Correct answer: {correct_answer}\")\n",
    "                print()\n",
    "\n",
    "            print(f\"You scored {score}/{len(questions)}! ({(score / len(questions)) * 100:.2f}%)\")\n",
    "            \n",
    "            # Show Continue or Try Again button based on score\n",
    "            if score >= 0.8 * len(questions):\n",
    "                continue_button = widgets.HTML(\n",
    "                    '<a href=\"case-study-6.ipynb\" style=\"display: inline-block; padding: 10px 15px; '\n",
    "                    'background-color: #28a745; color: white; text-decoration: none; border-radius: 5px;\">'\n",
    "                    'Continue</a>'\n",
    "                )\n",
    "                display(continue_button)\n",
    "            else:\n",
    "                try_again_button = widgets.HTML(\n",
    "                    '<a href=\"case-study-5.ipynb\" style=\"display: inline-block; padding: 10px 15px; '\n",
    "                    'background-color: #dc3545; color: white; text-decoration: none; border-radius: 5px;\">'\n",
    "                    'Score at least 80% to continue. Try Again</a>'\n",
    "                )\n",
    "                display(try_again_button)\n",
    "\n",
    "# Attach event to the submit button\n",
    "submit_button.on_click(on_submit_click)\n",
    "\n",
    "# Display the quiz\n",
    "for label, options in quiz_widgets:\n",
    "    display(label, options)\n",
    "display(submit_button, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***END CAP1*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[***TODO CAP2 - Proceed to Case Study***] Next: Case Study 6](case-study-6.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
