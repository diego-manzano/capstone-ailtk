{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import Google GenerativeAI Python module\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Define Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyBGOlsnd3I5J7-PrcxYOypZPb4wkAdrOxw\")\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Specify model name and define system instruction\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are a virtual restaurant and bar recommendation assistant. Your goal is to provide users with highly personalized recommendations based on their preferences and needs.\n",
    "\n",
    "Here are some guidelines to follow:\n",
    "\n",
    "- Understand the User's Intent: Carefully analyze the user's query.\n",
    "- Leverage User Preferences: Utilize the user's past behavior.\n",
    "- Consider Dietary Restrictions: Factor in dietary restrictions.\n",
    "- Provide Relevant Information: Offer details like cuisine type, price, and ambiance.\n",
    "- Handle Ambiguous Queries: Ask clarifying questions.\n",
    "- Be Conversational and Engaging: Maintain a friendly tone.\"\"\"\n",
    ")\n",
    "\n",
    "# Acceptable past chat for reference\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\"role\": \"user\", \"parts\": [\"Hello\"]},\n",
    "    {\"role\": \"model\", \"parts\": [\"Hello there! I am a virtual agent for Welp!\"]},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus successfully saved to corpus.pkl\n",
      "Corpus loaded from pickle file.\n",
      "These are Yelp reviews, not restaurant recommendations.  The reviews provide feedback on various food establishments:\n",
      "\n",
      "* **MaggieMoo's Ice Cream & Treatery (Nashville, TN):** Received a 5-star review for its waffle cone and mint ice cream.  The review is enthusiastic and focuses on the taste.  Note that the restaurant is listed as closed (`is_open: 0`).\n",
      "\n",
      "* **Christine Dahl (Santa Barbara, CA):**  A custom cake business received a 5-star review.  The customer raved about the cake's quality and the baker's pleasant demeanor.  The business is listed as closed (`is_open: 0`) and operates by appointment only.\n",
      "\n",
      "* **Chill Frozen Yogurt (Clayton, MO):** Received a 5-star review praising the wide variety of toppings and the quality of the yogurt.  The reviewer found the location \"quaint.\"  The shop has relatively long hours.\n",
      "\n",
      "* **Grandma's Cookies (St. Charles, MO):** Received a 5-star review highlighting the delicious cookies, hot chocolate, and friendly staff.  The business is currently open (`is_open: 1`).\n",
      "\n",
      "* **Cinnzeo (Edmonton, AB):** Received a 2-star review. The reviewer found the coffee inferior to Starbucks, even though they dislike Starbucks.  The review mentions an allergy preventing them from trying the pastries.  The business is currently open (`is_open: 1`).\n",
      "\n",
      "To provide restaurant *recommendations*, I need more information about your preferences (e.g., type of food, price range, location, atmosphere).  These reviews offer examples of positive and negative experiences but don't constitute a recommendation in themselves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the Excel file outputted in Case Study 3\n",
    "df = pd.read_excel(\"solution-case-study-activity-3/ailtk-case-apache-hop-output.xls\")\n",
    "\n",
    "# Combine relevant columns into a single document per row\n",
    "# Assuming the columns are 'input' and 'output', adjust if necessary\n",
    "corpus = df.apply(lambda row: f\"{row['input']}. {row['output']}\", axis=1).tolist()\n",
    "\n",
    "# Save corpus to a pickle file\n",
    "PICKLE_FILE = \"corpus.pkl\"\n",
    "\n",
    "with open(PICKLE_FILE, \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "print(f\"Corpus successfully saved to {PICKLE_FILE}\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "class RAGOrchestrator:\n",
    "    # Manages corpus loading, similarity calculations, and generating augmented responses using the LLM.\n",
    "\n",
    "    def __init__(self, pickle_file: str, model):\n",
    "        # Initializes the RAGOrchestrator.\n",
    "        # Parameters:\n",
    "        # - pickle_file (str): Path to the pickled corpus file.\n",
    "        # - model: Preloaded LLM instance for generating responses.\n",
    "\n",
    "        self.pickle_file = pickle_file\n",
    "        self.model = model\n",
    "        self.corpus = self._load_corpus()\n",
    "\n",
    "    def _load_corpus(self) -> List[str]:\n",
    "        # Loads the corpus from a pickle file.\n",
    "        \n",
    "        if not os.path.exists(self.pickle_file):\n",
    "            raise FileNotFoundError(f\"Pickle file '{self.pickle_file}' not found. Please generate it first.\")\n",
    "        \n",
    "        with open(self.pickle_file, \"rb\") as f:\n",
    "            print(\"Corpus loaded from pickle file.\")\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def _jaccard_similarity(query: str, document: str) -> float:\n",
    "        # Calculates Jaccard similarity between a query and a document.\n",
    "        \n",
    "        query_tokens = set(query.lower().split())\n",
    "        document_tokens = set(document.lower().split())\n",
    "        \n",
    "        intersection = query_tokens.intersection(document_tokens)\n",
    "        union = query_tokens.union(document_tokens)\n",
    "\n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "    def _get_similar_documents(self, query: str, top_n: int = 5) -> List[str]:\n",
    "        # Retrieves the top N most similar documents from the corpus.\n",
    "        \n",
    "        similarities = [self._jaccard_similarity(query, doc) for doc in self.corpus]\n",
    "        top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_n]\n",
    "        \n",
    "        return [self.corpus[i] for i in top_indices]\n",
    "\n",
    "    def generate_augmented_response(self, user_prompt: str) -> str:\n",
    "        # Generates a response using the LLM with an injected prompt from RAG results.\n",
    "\n",
    "        similar_docs = self._get_similar_documents(user_prompt)\n",
    "        injected_prompt = f\"{user_prompt} {' '.join(similar_docs)}\"\n",
    "\n",
    "        response = self.model.generate_content(injected_prompt)\n",
    "        return response.text\n",
    "\n",
    "# Example usage:\n",
    "PICKLE_FILE = \"corpus.pkl\"\n",
    "MODEL = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "orchestrator = RAGOrchestrator(PICKLE_FILE, MODEL)\n",
    "response = orchestrator.generate_augmented_response(\"Tell me about restaurant recommendations.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
