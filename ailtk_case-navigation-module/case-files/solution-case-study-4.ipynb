{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import Google GenerativeAI Python module\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Define Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyBGOlsnd3I5J7-PrcxYOypZPb4wkAdrOxw\")\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Specify model name and define system instruction\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"\"\"You are a virtual restaurant and bar recommendation assistant. Your goal is to provide users with highly personalized recommendations based on their preferences and needs.\n",
    "\n",
    "Here are some guidelines to follow:\n",
    "\n",
    "- Understand the User's Intent: Carefully analyze the user's query.\n",
    "- Leverage User Preferences: Utilize the user's past behavior.\n",
    "- Consider Dietary Restrictions: Factor in dietary restrictions.\n",
    "- Provide Relevant Information: Offer details like cuisine type, price, and ambiance.\n",
    "- Handle Ambiguous Queries: Ask clarifying questions.\n",
    "- Be Conversational and Engaging: Maintain a friendly tone.\"\"\"\n",
    ")\n",
    "\n",
    "# Acceptable past chat for reference\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\"role\": \"user\", \"parts\": [\"Hello\"]},\n",
    "    {\"role\": \"model\", \"parts\": [\"Hello there! I am a virtual agent for Welp!\"]},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus successfully saved to corpus.pkl\n",
      "Corpus loaded from pickle file.\n",
      "This data provides five restaurant reviews, each with varying aspects to consider for recommendation purposes.  Here's a breakdown:\n",
      "\n",
      "**Positive Reviews (5 stars):**\n",
      "\n",
      "* **MaggieMoo's Ice Cream & Treatery (Nashville, TN):**  A glowing review focusing solely on the deliciousness of the ice cream and waffle cone.  This is great for recommending to people looking for a sweet treat. However, note it's listed as closed (`is_open: 0`).\n",
      "\n",
      "* **Christine Dahl (Custom Cakes, Santa Barbara, CA):**  Excellent feedback on a custom cake, highlighting the baker's pleasant demeanor and the cake's exceptional quality.  This is ideal for recommending to those seeking a special occasion cake, but requires an appointment (`ByAppointmentOnly: True`).\n",
      "\n",
      "* **Chill Frozen Yogurt (Clayton, MO):**  Praises the cleanliness, variety of toppings, and overall quality of the frozen yogurt. This is a good recommendation for people looking for a self-serve frozen yogurt experience.  It has specified hours.\n",
      "\n",
      "* **Grandma's Cookies (St. Charles, MO):** Positive comments on the cookies, hot chocolate, and friendly staff.  A solid choice for those wanting a bakery item and hot beverage, and it is currently open (`is_open: 1`).  It has specified hours.\n",
      "\n",
      "**Negative Review (2 stars):**\n",
      "\n",
      "* **Cinnzeo (Edmonton, AB):**  A strongly negative review comparing the coffee unfavorably to Starbucks.  This establishment should *not* be recommended unless someone specifically requests cheap coffee and is indifferent to quality.\n",
      "\n",
      "\n",
      "**Recommendation Strategy:**\n",
      "\n",
      "To give the best recommendations, consider the following:\n",
      "\n",
      "* **User's Preferences:**  What kind of food are they looking for? Sweet treats, cakes, frozen yogurt, coffee, etc.?\n",
      "* **Location:** Where are they located?  Proximity is key.\n",
      "* **Opening Status:**  Check the `is_open` field.  Recommending a closed business is unhelpful.\n",
      "* **Specific Needs:**  Do they need takeout, delivery, parking, etc.? Check the attributes.\n",
      "* **Occasion:** Is it a casual snack or a special event?\n",
      "\n",
      "\n",
      "For example, if someone wants a delicious ice cream, MaggieMoo's *would have been* a great suggestion (if it were open). If someone needs a custom cake for a wedding, Christine Dahl is a strong contender.  If they are in Clayton, MO, and want frozen yogurt, Chill Frozen Yogurt is an excellent choice.  Grandma's Cookies is a good option if they're looking for a bakery with takeout options in St. Charles, MO.  Avoid Cinnzeo unless the user is very unconcerned about coffee quality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the Excel file outputted in Case Study 3\n",
    "df = pd.read_excel(\"solution-case-study-activity-3/ailtk-case-apache-hop-output.xls\")\n",
    "\n",
    "# Combine relevant columns into a single document per row\n",
    "# Assuming the columns are 'input' and 'output', adjust if necessary\n",
    "corpus = df.apply(lambda row: f\"{row['input']}. {row['output']}\", axis=1).tolist()\n",
    "\n",
    "# Save corpus to a pickle file\n",
    "PICKLE_FILE = \"corpus.pkl\"\n",
    "\n",
    "with open(PICKLE_FILE, \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "print(f\"Corpus successfully saved to {PICKLE_FILE}\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "class RAGOrchestrator:\n",
    "    # Manages corpus loading, similarity calculations, and generating augmented responses using the LLM.\n",
    "\n",
    "    def __init__(self, pickle_file: str, model):\n",
    "        # Initializes the RAGOrchestrator.\n",
    "        # Parameters:\n",
    "        # - pickle_file (str): Path to the pickled corpus file.\n",
    "        # - model: Preloaded LLM instance for generating responses.\n",
    "\n",
    "        self.pickle_file = pickle_file\n",
    "        self.model = model\n",
    "        self.corpus = self._load_corpus()\n",
    "\n",
    "    def _load_corpus(self) -> List[str]:\n",
    "        # Loads the corpus from a pickle file.\n",
    "        \n",
    "        if not os.path.exists(self.pickle_file):\n",
    "            raise FileNotFoundError(f\"Pickle file '{self.pickle_file}' not found. Please generate it first.\")\n",
    "        \n",
    "        with open(self.pickle_file, \"rb\") as f:\n",
    "            print(\"Corpus loaded from pickle file.\")\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def _jaccard_similarity(query: str, document: str) -> float:\n",
    "        # Calculates Jaccard similarity between a query and a document.\n",
    "        \n",
    "        query_tokens = set(query.lower().split())\n",
    "        document_tokens = set(document.lower().split())\n",
    "        \n",
    "        intersection = query_tokens.intersection(document_tokens)\n",
    "        union = query_tokens.union(document_tokens)\n",
    "\n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "    def _get_similar_documents(self, query: str, top_n: int = 5) -> List[str]:\n",
    "        # Retrieves the top N most similar documents from the corpus.\n",
    "        \n",
    "        similarities = [self._jaccard_similarity(query, doc) for doc in self.corpus]\n",
    "        top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_n]\n",
    "        \n",
    "        return [self.corpus[i] for i in top_indices]\n",
    "\n",
    "    def generate_augmented_response(self, user_prompt: str) -> str:\n",
    "        # Generates a response using the LLM with an injected prompt from RAG results.\n",
    "\n",
    "        similar_docs = self._get_similar_documents(user_prompt)\n",
    "        injected_prompt = f\"{user_prompt} {' '.join(similar_docs)}\"\n",
    "\n",
    "        response = self.model.generate_content(injected_prompt)\n",
    "        return response.text\n",
    "\n",
    "# Example usage:\n",
    "PICKLE_FILE = \"corpus.pkl\"\n",
    "MODEL = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "orchestrator = RAGOrchestrator(PICKLE_FILE, MODEL)\n",
    "response = orchestrator.generate_augmented_response(\"Tell me about restaurant recommendations.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
